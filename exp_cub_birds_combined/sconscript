import os
from os.path import join as path_join
from nestly.scons import SConsWrap
from nestly import Nest
from SCons.Script import Environment, Command, AddOption

"""
Dataset from https://data.caltech.edu/records/65de6-vp158
predict all CUB classes at once
"""

Import('env')
localenv = env.Clone()

# Set up state
nest = SConsWrap(Nest(), localenv['output'], alias_environment=localenv)

# NOTE: The LLM Api only supports images for OpenAI models

NUM_ITERS = 100
DATA_FOLDER = 'exp_cub_birds_existing/data/CUB_200_2011'
BATCH_SIZE = 200
BATCH_OBS_SIZE = 4
NUM_NEW_TOKENS = 3000
MAX_TRAIN_FRACS = [
    1
]

NUM_TOP_RESIDUAL_WORDS = 20
NUM_GREEDY_EPOCHS = 2
NUM_GREEDY_HOLDOUT = 10
NUM_BOOST_ITERS = 20
NUM_EPOCHS = 2
MAX_ITERS = 25

BAYESIAN_PRIOR_PROMPT = 'exp_cub_birds_combined/prompts/bayesian_prior.txt'
BAYESIAN_ITER_PROMPT = 'exp_cub_birds_combined/prompts/bayesian_iter.txt'
COMPARATOR_PROMPT = 'exp_cub_birds_combined/prompts/comparator_label_free_prompt.txt'
EXTRACT_LLM_PROMPT = 'exp_cub_birds_combined/prompts/extract_concepts_contrasts.txt'
BASELINE_PROMPT = 'exp_cub_birds_combined/prompts/baseline_init.txt'
BOOSTING_PROMPT = 'exp_cub_birds_combined/prompts/boosting_iter.txt'
CONCEPTS_PROMPT = 'exp_cub_birds_combined/prompts/concept_questions.txt'

MAX_TEST_OBS = 500

LLM_MODELS = [
    # 'gpt-4o-mini'
    'gpt-4o'
]
LLM_DICT = {
    'gpt-4o-mini': True, # true using API
    'gpt-4o': True, # true using API
}

nest.add_aggregate('res_agg_final', list)

@nest.add_target_with_env(localenv)
def assemble_cub(env, outdir, c):
    classes = " ".join([str(a) for a in range(1,201)])
    cmd = [
        'python scripts/assemble_cub_birds.py',
        '--keep-classes', classes,
        '--dataset-folder',
        DATA_FOLDER,
        '--labelled-data ${TARGETS[0]}',
    ]

    targets = [
        path_join(outdir, 'labels.csv')
    ]

    return env.Command(
        targets,
        [],
        ' '.join(map(str, cmd))
    )

nest.add('seed', [
        0
    ],
    label_func=lambda c: "seed_%d" % c
)

nest.add('test_frac',
    [
        0.5
    ],
    label_func=lambda c: "test_%.2f" % c
)

@nest.add_target_with_env(localenv)
def train_test_split(env, outdir, c):
    cmd = [
        'python scripts/train_test_split.py',
        '--seed', c['seed'] + 1,
        '--data-csv ${SOURCES[0]}',
        '--test-frac',
        c['test_frac'],
        '--indices-csv ${TARGETS[0]}',
    ]

    sources = c['assemble_cub']
    targets = [
        path_join(outdir, 'train_test_indices.csv'),
    ]

    return env.Command(
        targets,
        sources,
        ' '.join(map(str, cmd))
    )

nest.add('llm_model', LLM_MODELS)

@nest.add_target_with_env(localenv)
def extract_llm_output(env, outdir, c):
    targets = [
        path_join(outdir, 'log_extract.txt'),
        path_join(outdir, 'concept_extractions1.csv'),
    ]
    cmd = [
        'python scripts/extract_llm_concepts.py',
        f'--seed',
        c['seed'],
        '--is-image',
        '--batch-obs-size',
        BATCH_OBS_SIZE,
        '--batch-size',
        40, #BATCH_SIZE,
        '--num-new-tokens',
        NUM_NEW_TOKENS,
        '--in-dataset-file ${SOURCES[0]}',
        '--indices ${SOURCES[1]}',
        '--prompt-file', EXTRACT_LLM_PROMPT,
        '--log-file ${TARGETS[0]}',
        '--llm-output ${TARGETS[1]}',
        '--llm-model-type',
        c['llm_model'],
        '--use-api' if LLM_DICT[c['llm_model']] else '',
    ]

    sources = [
        c['assemble_cub'][0],
        c['train_test_split'][0]
    ]

    return env.Command(
        targets,
        sources,
        ' '.join(map(str, cmd))
    )

nest.add('num_meta_concepts', 
    [
        100
    ],
    label_func=lambda c: "num_concepts_%s" % c
)
nest.add_aggregate('all_test_extractions', list)
nest.add_aggregate('bayesian_agg', list)
nest.add(
    'max_train_frac',
    MAX_TRAIN_FRACS,
    label_func=lambda c: "max_train_frac_%.2f" % c
)
nest.add_aggregate('result_agg', list)
nest.add_aggregate('train_baseline_history', str)


nest.add(
    'batch_concept_size',
    [
        20
    ],
    label_func=lambda c: "batch_concept_%d" % c)

@nest.add_target_with_env(localenv)
def evaluate_comparator(env, outdir, c):
    # evaluating label-free CBMs
    cmd = [
        'python scripts/evaluate_comparator.py',
        f'--seed',
        c['seed'] + 5,
        '--batch-obs-size',
        BATCH_OBS_SIZE,
        '--batch-size',
        BATCH_SIZE,
        '--batch-concept-size', c['batch_concept_size'] * 3,
        '--in-dataset-file ${SOURCES[0]}',
        '--indices-csv ${SOURCES[1]}',
        '--log-file ${TARGETS[0]}',
        '--llm-model-type',
        c['llm_model'],
        '--prompt-concepts-file', COMPARATOR_PROMPT,
        '--result-csv ${TARGETS[1]}',
        '--out-extraction ${TARGETS[2]}',
    ]

    sources = [
        c['assemble_cub'][0],
        c['train_test_split'][0],
    ]
    targets = [
        path_join(outdir, 'comparator_test_log.txt'),
        path_join(outdir, 'comparator_result.csv'),
        path_join(outdir, 'comparator_extract.pkl'),
    ]
    c['result_agg'].append(targets[1])

    return env.Command(
        targets,
        sources,
        ' '.join(map(str, cmd))
    )

@nest.add_target_with_env(localenv)
def train_baseline(env, outdir, c):
    cached_extractions = path_join('exp_cub_birds_combined', outdir, 'baseline_extractions.pkl')
    cmd = [
        'python scripts/train_baseline.py',
        f'--seed',
        c['seed'] + 2,
        '--batch-size', BATCH_SIZE,
        '--batch-concept-size', c['batch_concept_size'],
        '--batch-obs-size',
        BATCH_OBS_SIZE,
        '--min-prevalence', 0.003,
        '--num-meta-concepts',
        c['num_meta_concepts'],
        '--learner-type count_l1',
        '--final-learner l1',
        '--num-top-attributes',
        c['num_meta_concepts'] + 20,
        '--in-dataset-file ${SOURCES[0]}',
        '--indices-csv ${SOURCES[1]}',
        '--out-training-history-file ${TARGETS[0]}',
        '--log-file  ${TARGETS[1]}',
        '--llm-model-type',
        'gpt-4o',
         '--use-api',
         '--out-extractions', cached_extractions,
         '--is-image',
         '--use-acc',
         '--prompt-concepts-file', CONCEPTS_PROMPT,
         '--baseline-init-file', BASELINE_PROMPT,
         '--init-concepts-file ${SOURCES[2]}'
    ]

    sources = [
        c['assemble_cub'][0],
        c['train_test_split'][0],
        c['extract_llm_output'][1],
    ]
    targets = [
        path_join(outdir, 'baseline_history.pkl'),
        path_join(outdir, 'log_train_baseline_new.txt'),
    ]
    c['train_baseline_history'] = targets[0]

    return env.Command(
        targets,
        sources,
        ' '.join(map(str, cmd))
    )

@nest.add_target_with_env(localenv)
def train_boosting(env, outdir, c):
#    cached_extractions = path_join('exp_cub', outdir, 'boosting_extractions.pkl')
    cmd = [
        'python scripts/train_boosting.py',
        f'--seed',
        c['seed'] + 2,
        '--num-boost-samples', 2,
        '--num-iters', NUM_BOOST_ITERS,
        '--batch-size', BATCH_SIZE,
        '--boosting-prompt', BOOSTING_PROMPT,
        '--prompt-concepts-file', CONCEPTS_PROMPT,
        '--in-dataset-file ${SOURCES[0]}',
        '--indices-csv ${SOURCES[1]}',
        '--out-training-history-file ${TARGETS[0]}',
        '--log-file  ${TARGETS[1]}',
        '--llm-model-type', c['llm_model'],
        '--use-api' if LLM_DICT[c['llm_model']] else '',
#        '--out-extractions', cached_extractions,
         '--is-image',
    ]

    sources = [
        c["assemble_cub"][0],
        c['train_test_split'][0],
    ]
    targets = [
        path_join(outdir, 'boosting_history.pkl'),
        path_join(outdir, 'log_train_boosting.txt')
    ]
    c['train_boosting_history'] = targets[0]
    if os.path.exists(targets[0]):
        return

    return env.Command(
        targets,
        sources,
        ' '.join(map(str, cmd))
    )

@nest.add_target_with_env(localenv)
def evaluate_boosting(env, outdir, c):
    cmd = [
        'python scripts/evaluate_bayesian.py',
        f'--seed',
        c['seed'] + 5,
        '--method-name boosting',
        '--num-posterior-iters',
        1,
        '--batch-size',
        BATCH_SIZE,
        '--prompt-concepts-file', CONCEPTS_PROMPT,
        '--in-dataset-file ${SOURCES[0]}',
        '--indices-csv ${SOURCES[1]}',
        '--training-history-file ${SOURCES[2]}',
#        '--out-extraction ${SOURCES[3]}',
        '--log-file ${TARGETS[0]}',
        '--llm-model-type',
        c['llm_model'],
        '--use-api' if LLM_DICT[c['llm_model']] else '',
        '--result-csv ${TARGETS[1]}',
        # '--calib ${TARGETS[1]}',
        '--is-image',
    ]

    sources = [
        c["assemble_cub"][0],
        c['train_test_split'][0],
        c['train_boosting_history'],
#        c['test_extractions'],
    ]
    targets = [
        path_join(outdir, 'test_boosting_log1.txt'),
        # path_join(outdir, 'calib_boosting.png'),
        path_join(outdir, 'result_boosting.csv'),
    ]
    c['result_agg'].append(targets[1])

    return env.Command(
        targets,
        sources,
        ' '.join(map(str, cmd))
    )

# @nest.add_target_with_env(localenv)
# def train_blackbox(env, outdir, c):
#     cmd = [
#             'python scripts/train_image_blackbox.py',
#             '--seed', c['seed'] + 2,
#             '--in-dataset-file ${SOURCES[0]}',
#             '--indices-csv ${SOURCES[1]}',
#             '--log-file  ${TARGETS[0]}',
#             '--out-mdl ${TARGETS[1]}'
#     ]

#     sources = [
#         c['assemble_cub'][0],
#         c['train_test_split'][0],
#     ]
#     targets = [
#         path_join(outdir, 'log_train_image_resnet.txt'),
#         path_join(outdir, 'resnet_image_mdl.pkl'),
#     ]

#     return env.Command(
#         targets,
#         sources,
#         ' '.join(map(str, cmd))
#     )


# @nest.add_target_with_env(localenv)
# def evaluate_blackbox(env, outdir, c):
#     cmd = [
#             'python scripts/evaluate_image_blackbox.py',
#             f'--seed', c['seed'] + 5,
#             '--in-dataset-file ${SOURCES[0]}',
#             '--indices-csv ${SOURCES[1]}',
#             '--in-mdl ${SOURCES[2]}',
#             '--results-csv ${TARGETS[0]}'
#     ]

#     sources = [
#         c['assemble_cub'][0],
#         c['train_test_split'][0],
#         c['train_blackbox'][1]
#     ]
#     targets = [
#         path_join(outdir, 'image_resnet_results.csv'),
#     ]
#     c['result_agg'].append(targets[0])

#     return env.Command(
#         targets,
#         sources,
#         ' '.join(map(str, cmd))
#     )

nest.add(
    'train_frac',
    [
        0.75
    ],
    label_func=lambda c: "train_frac_%.2f" % c)

nest.add(
    'minibatch',
    [
        None
    ],
    label_func=lambda c: "minibatch_%s" % str(c))

nest.add_aggregate('train_bayesian_history', str)
@nest.add_target_with_env(localenv)
def train_bayesian(env, outdir, c):
    cached_extractions = path_join('exp_cub_birds_combined', outdir, 'extractions.pkl')
    cmd = [
        'python scripts/train_bayesian.py',
        f'--seed',
        c['seed'] + 4,
        '--batch-size', BATCH_SIZE,
        '--batch-concept-size', c['batch_concept_size'],
        '--batch-obs-size',
        BATCH_OBS_SIZE,
        '--num-greedy-epochs', NUM_GREEDY_EPOCHS,
        '--num-greedy-holdout', NUM_GREEDY_HOLDOUT,
        '--is-greedy-metric-acc',
        '--min-prevalence', 0.003,
        '--max-epochs',
        NUM_EPOCHS,
        '--num-meta-concepts',
        c['num_meta_concepts'],
        '--learner-type count_l1',
        '--final-learner l1',
        '--train-frac', c['train_frac'],
        '--in-dataset-file ${SOURCES[0]}',
        '--indices-csv ${SOURCES[2]}',
        '--out-extraction', cached_extractions,
        '--log-file ${TARGETS[0]}',
        '--init-history-file ${SOURCES[1]}',
        '--training-history-file ${TARGETS[1]}',
        '--aucs-plot-file ${TARGETS[2]}',
        '--llm-model-type',
        c['llm_model'],
        '--use-api' if LLM_DICT[c['llm_model']] else '',
        '--is-image',
        '--prompt-concepts-file', CONCEPTS_PROMPT,
        '--prompt-prior-file', BAYESIAN_PRIOR_PROMPT,
        '--prompt-iter-file', BAYESIAN_ITER_PROMPT,
        '--prompt-iter-type conditional',
        '--num-top-residual-words', NUM_TOP_RESIDUAL_WORDS,
        '--init-concepts-file ${SOURCES[3]}',
    ]
    if c['minibatch']:
        cmd += ['--num-minibatch', c['minibatch']]

    sources = [
        c['assemble_cub'][0],
        c['train_baseline_history'],
        c['train_test_split'][0],
        c['extract_llm_output'][1],
    ]
    targets = [
        path_join(outdir, 'log_train_bayesian1.txt'),
        path_join(outdir, 'training_history.pkl'),
        path_join(outdir, 'aucs.png'),
    ]
    c['train_bayesian_history'] = targets[1]
    c['bayesian_agg'].append(targets[1])
    return env.Command(
        targets,
        sources,
        ' '.join(map(str, cmd))
        )

@nest.add_target_with_env(localenv)
def evaluate_bayesian(env, outdir, c):
    cmd = [
        'python scripts/evaluate_bayesian.py',
        f'--seed',
        c['seed'] + 5,
        '--num-posterior-iters',
        NUM_EPOCHS * c['num_meta_concepts'],
        '--batch-size',
        BATCH_SIZE,
        '--batch-concept-size', c['batch_concept_size'],
        '--batch-obs-size',
        BATCH_OBS_SIZE,
        '--max-obs', MAX_TEST_OBS,
        '--method-name bayesian',
        '--in-dataset-file ${SOURCES[0]}',
        '--indices-csv ${SOURCES[1]}',
        '--training-history-file ${SOURCES[2]}',
        '--log-file ${TARGETS[0]}',
        '--llm-model-type',
        c['llm_model'],
        '--prompt-concepts-file', CONCEPTS_PROMPT,
        '--use-api' if LLM_DICT[c['llm_model']] else '',
        '--is-image',
        '--result-csv ${TARGETS[1]}',
        '--out-extraction ${TARGETS[2]}',
    ]

    sources = [
        c['assemble_cub'][0],
        c['train_test_split'][0],
        c['train_bayesian_history']
    ]
    targets = [
        path_join(outdir, 'test_bayesian_log_new.txt'),
        path_join(outdir, 'result_bayesian_new.csv'),
        path_join(outdir, 'extractions_bayesian_new.pkl'),
    ]
    c['result_agg'].append(targets[1])

    return env.Command(
        targets,
        sources,
        ' '.join(map(str, cmd))
    )

